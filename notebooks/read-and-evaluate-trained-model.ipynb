{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore a Trained Model\n",
    "In order to assess your model, you must [pickle its state](train-and-pickle-model.ipnb), so that we can evaluate it using your implementation of `classify()`, which we provide an example using the [linked example model](train-and-pickle-model.ipnb).\n",
    "\n",
    "Your `classify()` implementation should instantiate your trained model and read the array of matrix glyphs given to it.  You can use the example below to get you started.\n",
    "\n",
    "We'll start by importing some prerequisites and some code from the course to read test data.  We include a couple sample glyphs from each letter to demonstrate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "from scipy import ndimage\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run udacity-tensorflow-support.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read images from the samples directory.  Feel free to add your own 28x28 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bredi\\Documents\\Projects\\AT-AI-Challenge\\notebooks\\samples\\A.pickle already present - Skipping pickling.\n",
      "C:\\Users\\bredi\\Documents\\Projects\\AT-AI-Challenge\\notebooks\\samples\\B.pickle already present - Skipping pickling.\n",
      "C:\\Users\\bredi\\Documents\\Projects\\AT-AI-Challenge\\notebooks\\samples\\C.pickle already present - Skipping pickling.\n",
      "C:\\Users\\bredi\\Documents\\Projects\\AT-AI-Challenge\\notebooks\\samples\\D.pickle already present - Skipping pickling.\n",
      "C:\\Users\\bredi\\Documents\\Projects\\AT-AI-Challenge\\notebooks\\samples\\E.pickle already present - Skipping pickling.\n",
      "C:\\Users\\bredi\\Documents\\Projects\\AT-AI-Challenge\\notebooks\\samples\\F.pickle already present - Skipping pickling.\n",
      "C:\\Users\\bredi\\Documents\\Projects\\AT-AI-Challenge\\notebooks\\samples\\G.pickle already present - Skipping pickling.\n",
      "C:\\Users\\bredi\\Documents\\Projects\\AT-AI-Challenge\\notebooks\\samples\\H.pickle already present - Skipping pickling.\n",
      "C:\\Users\\bredi\\Documents\\Projects\\AT-AI-Challenge\\notebooks\\samples\\I.pickle already present - Skipping pickling.\n",
      "C:\\Users\\bredi\\Documents\\Projects\\AT-AI-Challenge\\notebooks\\samples\\J.pickle already present - Skipping pickling.\n",
      "Testing size (20, 784) (20, 10)\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'samples'\n",
    "\n",
    "glyph_dir = Path.cwd().joinpath(dir_name)\n",
    "test_folders = [glyph_dir.joinpath(i) for i in glyph_dir.iterdir()]\n",
    "test_folders.sort()\n",
    "test_datasets = maybe_pickle(test_folders, 2) # provide only 2 samples for now\n",
    "test_dataset, test_labels = merge_datasets(test_datasets, 20)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Testing size', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our `classify()` example, with some utility code to help pack the testing data to fit into the pickled tensors.  The labels `test_data:0` and `test_predictor:0` must match those named in the pickled model, [e.g. the example](train-and-pickle-model.ipnb).  Most of the complexity stems from the testing input batch size might not match the training input batch size, so we pad the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_data(tensor_shape, dataset=None, labels=None):\n",
    "    \"\"\"\n",
    "    Pad the dataset and labels to fit the pickled input tensor size.\n",
    "    If tensor_shape is smaller than the input to pad, data are truncated.\n",
    "    \n",
    "    Args:\n",
    "        tensor_shape: 2-element array or tuple of the shape to pad to.\n",
    "        dataset: the input data to pad, ignored if omitted.\n",
    "        labels: the labels to pad, ignored if omitted.\n",
    "    \n",
    "    Returns:\n",
    "        The pair of padded data and labels, with the placeholder None if\n",
    "        the respective input were omitted.\n",
    "    \"\"\"\n",
    "    n = int(tensor_shape[0])\n",
    "    m = int(dataset.shape[0])\n",
    "    \n",
    "    result_data = None\n",
    "    if dataset is not None:\n",
    "        result_data = np.ndarray(tensor_shape, dtype=np.float32)\n",
    "    \n",
    "    result_labels = None\n",
    "    if labels is not None:\n",
    "        result_labels = np.ndarray((n, labels.shape[1]), dtype=np.int32)\n",
    "        \n",
    "    for i in range(0, int(math.ceil(n / m))):\n",
    "        start = i*m\n",
    "        end = min(n, ((i+1)*m))\n",
    "        if dataset is not None:\n",
    "            result_data[start:end, :] = dataset[0:end, :]\n",
    "        if labels is not None:\n",
    "            result_labels[start:end, :] = labels[0:end, :]\n",
    "    return result_data, result_labels\n",
    "\n",
    "\n",
    "def classify(test_dataset, model_filename='model.meta', checkpoint_path=None):\n",
    "    \"\"\"\n",
    "    A sample classifier to unpickle a TensorFlow model and label the dataset.\n",
    "    There are magic strings in this function derived from to the model to evaluate.\n",
    "    Your implementation will likely have different tags that depend upon the model\n",
    "    implementation.\n",
    "    \n",
    "    We pad the input test_dataset to make it at least as large as the model batch,\n",
    "    and repeat prediction on chunks of the input if it is larger than the model batch.\n",
    "    \n",
    "    Args:\n",
    "        test_dataset: the #observations by 768 feature nd-array glyphs to classify.\n",
    "        model_filename: optional file name stored by a previous TensorFlow session.\n",
    "        checkpoint_path: optional path for previous TensorFlow session.\n",
    "        \n",
    "    Returns:\n",
    "        The #observations by #labels nd-array labelings.\n",
    "    \"\"\"\n",
    "    num_classes = 10\n",
    "    n = int(test_dataset.shape[0])\n",
    "    result = np.ndarray([n, num_classes], dtype=np.int32)\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        saver = tf.train.import_meta_graph(model_filename)\n",
    "        if not checkpoint_path:\n",
    "            checkpoint_path = Path.cwd()\n",
    "        saver.restore(session, tf.train.latest_checkpoint(checkpoint_path))\n",
    "        graph_predict = tf.get_default_graph()\n",
    "        test_predict = graph_predict.get_tensor_by_name('test_predictor:0') # string from model\n",
    "        m = int(graph_predict.get_tensor_by_name('test_data:0').shape[0])   # string from model\n",
    "        \n",
    "        for i in range(0, int(math.ceil(n / m))):\n",
    "            start = i*m\n",
    "            end = min(n, ((i+1)*m))\n",
    "            test_dataset_pad, _ = pad_data(\n",
    "                graph_predict.get_tensor_by_name('test_data:0').shape,     # string from model\n",
    "                dataset=test_dataset[start:end, :])\n",
    "            result[start:end, :] = test_predict.eval(feed_dict={\"test_data:0\" : test_dataset_pad})[0:(end-start), :]\n",
    "    return result[0:test_dataset.shape[0], 0:test_dataset.shape[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, here is how we would use the `classify()` function to evaluate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\bredi\\Documents\\Projects\\AT-AI-Challenge\\notebooks/model\n",
      "Test accuracy: 50.0%\n"
     ]
    }
   ],
   "source": [
    "predict = classify(test_dataset)\n",
    "print(\"Test accuracy: %.1f%%\" % accuracy(predict, test_labels))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
