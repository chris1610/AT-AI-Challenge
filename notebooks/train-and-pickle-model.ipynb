{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Model Training and Saving\n",
    "Here is a sample model to classify notMNIST data and save the trained weights.  **Spoiler alert:** the following reveals simple solution for one of the Udacity assignments.\n",
    "\n",
    "The sections pertinent to saving your model are highlighted in the final two sections with Python comments.  You should label the input and output tensors of your model with the optional name parameter.  An instance of [tf.train.Saver](https://www.tensorflow.org/api_docs/python/tf/train/Saver) can write out the model and TensorFlow state to a file.\n",
    "\n",
    "In this example, the files will be named `model.data-00000-of-00001`, `model.index`, and `checkpoint`.  You must include both your model and checkpoint files in your submission to recover your model state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data input from [Udacity coursework](https://www.udacity.com/course/deep-learning--ud730) reading the pickled training data.\n",
    "\n",
    "*We do not include the data (`notMINST.pickle` file) in this repository.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    \n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "Here is a sample model prepared for saving to disk, so that we can use the training results later.  The pertinent aspect here is to make sure to label any tensor that we will read or write to during evaluation using the optional `name` arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden = 1024\n",
    "\n",
    "sgd_hidden_graph = tf.Graph()\n",
    "with sgd_hidden_graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    # ---------------------------------------> LABELED FOR LATER USE <----------------\n",
    "    tf_test_dataset = tf.constant(test_dataset, name='test_data')\n",
    "    \n",
    "    w0 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden]), name='W0')\n",
    "    w1 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]), name='W1')\n",
    "\n",
    "    b0 = tf.Variable(tf.zeros([num_hidden]), name='b0')\n",
    "    b1 = tf.Variable(tf.zeros([num_labels]), name='b1')\n",
    "\n",
    "    def reluLayer(dataset):\n",
    "        return tf.nn.relu(tf.matmul(dataset, w0) + b0)\n",
    "    def logitLayer(dataset):\n",
    "        return tf.matmul(reluLayer(dataset), w1) + b1\n",
    "    \n",
    "    sgd_hidden_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf_train_labels, \n",
    "            logits=logitLayer(tf_train_dataset)))\n",
    "    sgd_hidden_optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(sgd_hidden_loss)\n",
    "  \n",
    "    # ---------------------------------------------------------------------> LABELED FOR LATER USE <--\n",
    "    sgd_hidden_train_prediction = tf.nn.softmax(logitLayer(tf_train_dataset), name='train_predictor')\n",
    "    sgd_hidden_valid_prediction = tf.nn.softmax(logitLayer(tf_valid_dataset), name='validate_predictor')\n",
    "    sgd_hidden_test_prediction = tf.nn.softmax(logitLayer(tf_test_dataset), name='test_predictor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "We demonstrate how to write the model state to disk.  You will need to save the `model*` and `checkpoint` files for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 302.069550\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 29.4%\n",
      "Minibatch loss at step 500: 15.300731\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 1000: 12.396424\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 1500: 5.337847\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2000: 3.442477\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 2500: 3.387420\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 3000: 1.928396\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.2%\n",
      "Test accuracy: 89.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=sgd_hidden_graph) as sgd_hidden_session:\n",
    "    # ------> CREATE A SAVER INSTANCE TO PICKLE YOUR MODEL <---\n",
    "    saver = tf.train.Saver()\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :].reshape(batch_size, num_labels)\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, sgd_hidden_predictions = sgd_hidden_session.run(\n",
    "            [sgd_hidden_optimizer, sgd_hidden_loss, sgd_hidden_train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(sgd_hidden_predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(sgd_hidden_valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(sgd_hidden_test_prediction.eval(), test_labels))\n",
    "    # -----------------> MODEL IS SAVED HERE <-------------------\n",
    "    saver.save(sgd_hidden_session, '{}/model'.format(Path.cwd()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
